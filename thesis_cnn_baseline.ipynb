{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "thesis_cnn_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xurAS0-SKWda",
        "colab_type": "text"
      },
      "source": [
        "Code based in part on the CNN described by Hashemi et al.,\n",
        "\n",
        "```\n",
        "Homa B Hashemi et al. \"Query intent detection using convolutional neural\n",
        "networks\". In: International Conference on Web Search and Data Mining,\n",
        "Workshop on Query Understanding. 2016.\n",
        "```\n",
        "\n",
        "and on other TextCNN implementations by Yoon Kim (https://github.com/yoonkim/CNN_sentence) and Shawn Ng (https://github.com/Shawn1993/cnn-text-classification-pytorch)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h4IWSm7xiSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ensure spacy is at v. 2.0.11 (import especially for Italian vectors!) Env restart likely required.\n",
        "!pip install --upgrade spacy==2.0.11"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WHzRcEnxj2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m spacy validate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-OzD4cUw76s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this and next cell for IT vectors (might require restarting the env after running this cell)\n",
        "!pip3 install https://github.com/MartinoMensio/it_vectors_wiki_spacy/releases/download/v1.0/it_vectors_wiki_lg-1.0.0.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gVFLwt663xF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from spacy.attrs import ID\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from time import time\n",
        "from torch.nn.functional import relu, max_pool1d, log_softmax, cross_entropy  #, max_pool2d\n",
        "from torch.optim import SGD, Adam\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from typing import Tuple, List, Dict\n",
        "from tqdm import tqdm, trange\n",
        "# from tqdm import tqdm_notebook as tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qlvG6Pt8U65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUlxfXuF8WuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            embeddings=None,\n",
        "            num_embeddings: int = 52,  # number of features\n",
        "            embedding_dim: int = 300,  # dimensionality of embeddings\n",
        "            num_classes: int = 7,  # number of output classes\n",
        "            num_filters: int = 3,\n",
        "            filter_sizes: Tuple[int] = (2, 3, 4),\n",
        "            dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        if embeddings is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(embeddings, freeze=False)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(num_embeddings + 1, embedding_dim)\n",
        "\n",
        "        self.convs = nn.ModuleList(\n",
        "            [nn.Conv2d(1, num_filters, (fs, embedding_dim)) for fs in filter_sizes])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        x = [relu(conv(x)).squeeze(3) for conv in self.convs]\n",
        "\n",
        "        x = [max_pool1d(i, i.size(2)) for i in x]\n",
        "        x = torch.cat(x, 1).squeeze(2)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return self.fc(x)  # logit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h38vZAkA8Z4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm_notebook as tqdm\n",
        "def train(model: CNN, train_data: DataLoader, validation_data: DataLoader,\n",
        "          num_epochs=100, lr=0.1):\n",
        "    if 'cuda' == device:\n",
        "        model.cuda()\n",
        "\n",
        "    # optimizer = SGD(model.parameters(), lr=lr)\n",
        "    optimizer = Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0.0\n",
        "    tr_steps = 0\n",
        "    # inter_epoch_steps = 0\n",
        "\n",
        "    model.zero_grad()\n",
        "    # validation_accuracy, last_validation_accuracy = 0.0, 0.0\n",
        "    # train_iterator = trange(num_epochs, desc='Epoch')\n",
        "    train_iterator = range(num_epochs)\n",
        "    for i in train_iterator:\n",
        "        model.train()\n",
        "        epoch_iterator = tqdm(train_data, desc='Iteration')\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input = {'input_ids': batch[0],\n",
        "                     'labels': batch[1]}\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input['input_ids'])\n",
        "\n",
        "            loss = cross_entropy(logits, input['labels'])\n",
        "            loss.backward()\n",
        "            tr_loss += loss.item()\n",
        "\n",
        "            optimizer.step()\n",
        "            model.zero_grad()\n",
        "            tr_steps += 1\n",
        "\n",
        "            # if inter_epoch_steps % 50:\n",
        "            #   print()\n",
        "            # else:\n",
        "            #   print('.', end='')\n",
        "\n",
        "        # Validation\n",
        "        # last_validation_accuracy = validation_accuracy\n",
        "        validation_accuracy = evaluate(model, validation_data)\n",
        "        print('epoch {} (steps: {}) --- loss: {:.5f} --- '\n",
        "              'acc: {:.5f}'.format(\n",
        "            i, tr_steps, tr_loss / tr_steps, validation_accuracy))\n",
        "        \n",
        "        # if validation_accuracy < last_validation_accuracy or min_delta > validation_accuracy - last_validation_accuracy:\n",
        "        #   if i % 10 != 0:\n",
        "        #     print('epoch {} (steps: {}) --- loss: {:.5f} --- '\n",
        "        #           'acc: {:.5f}'.format(\n",
        "        #         i, tr_steps, tr_loss / tr_steps, validation_accuracy))\n",
        "        #   return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAssM5sW8bwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def score(y_true, y_pred, scoring='average'):\n",
        "  if 'f1-micro' == scoring:\n",
        "    return f1_score(y_true, y_pred, average='micro')\n",
        "\n",
        "  if 'f1-macro' == scoring:\n",
        "    return f1_score(y_true, y_pred, labels=np.unique(y_pred), average='macro')\n",
        "  \n",
        "  return (y_true == y_pred).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAIxBpdO8dI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, validation_data, scoring='average', predict=False):\n",
        "    eval_acc = 0.0\n",
        "    eval_steps = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    model.eval()\n",
        "    for batch in validation_data:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {'input_ids': batch[0],\n",
        "                      'labels': batch[1]}\n",
        "            logits = model(inputs['input_ids'])\n",
        "\n",
        "        pred = logits.detach().cpu().numpy()\n",
        "        label_ids = inputs['labels'].detach().cpu().numpy()\n",
        "\n",
        "        pred = np.argmax(pred, axis=1)\n",
        "        eval_acc += score(y_true=label_ids, y_pred=pred, scoring=scoring)\n",
        "        eval_steps += 1\n",
        "\n",
        "        y_true.extend(label_ids)\n",
        "        y_pred.extend(pred)\n",
        "\n",
        "    if predict:\n",
        "      return eval_acc / eval_steps, y_true, y_pred\n",
        "\n",
        "    return eval_acc / eval_steps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUkquU5Q8fS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 53\n",
        "\n",
        "# Uncomment one TRAIN_DATA_PATH and one TEST_DATA_PATH, along with the appropriate SPACY_MODEL\n",
        "\n",
        "# TRAIN_DATA_PATH = 'snips_train.tsv'\n",
        "# TRAIN_DATA_PATH = 'snips_small_train.tsv'\n",
        "# TEST_DATA_PATH = 'snips_validate.tsv'\n",
        "# TRAIN_DATA_PATH = 'atis_train.tsv'\n",
        "# TRAIN_DATA_PATH = 'atis_small_train.tsv'\n",
        "# TEST_DATA_PATH = 'atis_dev.tsv'\n",
        "# TRAIN_DATA_PATH = 'aw_slu_train.tsv'\n",
        "# TRAIN_DATA_PATH = 'aw_slu_small_train.tsv'\n",
        "# TEST_DATA_PATH = 'aw_slu_test.tsv'\n",
        "\n",
        "# SPACY_MODEL = 'en_vectors_web_lg'\n",
        "# SPACY_MODEL = 'it_vectors_wiki_lg'\n",
        "\n",
        "if 'it_vectors_wiki_lg' == SPACY_MODEL:\n",
        "  try:\n",
        "    import it_vectors_wiki_lg\n",
        "    nlp = it_vectors_wiki_lg.load()\n",
        "  except:\n",
        "    raise Exception('Italian GloVe vectors not available in this environment')\n",
        "else:\n",
        "  try:\n",
        "      nlp = spacy.load(SPACY_MODEL)\n",
        "  except:\n",
        "      spacy.cli.download(SPACY_MODEL)\n",
        "      nlp = spacy.load(SPACY_MODEL)\n",
        "\n",
        "df = pd.read_csv(TRAIN_DATA_PATH, delimiter='\\t', header=None,\n",
        "                    names=['sentence', 'intent']).dropna(how='any')\n",
        "\n",
        "unique_labels = sorted(list(set(df.intent)))\n",
        "print('labels:', unique_labels)\n",
        "\n",
        "train_x = [sentence.lower() for sentence in df.sentence]\n",
        "train_y = [unique_labels.index(intent) for intent in df.intent]\n",
        "\n",
        "print('length train_x, train_y:', len(train_x), len(train_y))\n",
        "\n",
        "embedding_matrix = nlp.vocab.vectors.data\n",
        "\n",
        "for idx, sentence in enumerate(train_x):\n",
        "    toks = nlp(sentence)\n",
        "    x = toks.to_array([ID])\n",
        "    x.dtype = 'long'\n",
        "    train_x[idx] = torch.tensor(x)\n",
        "\n",
        "train_x = pad_sequences(train_x, maxlen=MAX_SEQUENCE_LENGTH, dtype='long',\n",
        "                        padding='post', truncating='post')\n",
        "\n",
        "train_x, validate_x, train_y, validate_y = train_test_split(\n",
        "    train_x, train_y, shuffle=True, test_size=0.1)\n",
        "\n",
        "train_x = torch.tensor(train_x)\n",
        "train_y = torch.tensor(train_y)\n",
        "validate_x = torch.tensor(validate_x)\n",
        "validate_y = torch.tensor(validate_y)\n",
        "\n",
        "train_dataset = TensorDataset(train_x, train_y)\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, num_workers=2, batch_size=50)\n",
        "\n",
        "validate_dataset = TensorDataset(validate_x, validate_y)\n",
        "validate_dataloader = DataLoader(validate_dataset)\n",
        "\n",
        "cnn = CNN(embeddings=torch.tensor(embedding_matrix), num_classes=len(unique_labels))\n",
        "cnn = cnn.float()\n",
        "\n",
        "t1 = time()\n",
        "train(cnn, train_dataloader, validate_dataloader, num_epochs=100)\n",
        "t2 = time()\n",
        "\n",
        "print(f'\\n{t2 - t1}\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxvdjr0M9j3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = pd.read_csv(TEST_DATA_PATH, delimiter='\\t', header=None,\n",
        "                    names=['sentence', 'intent']).dropna(how='any')\n",
        "test_x = [sentence.lower() for sentence in test_df.sentence]\n",
        "test_y = [unique_labels.index(intent) for intent in test_df.intent]\n",
        "\n",
        "for idx, sentence in enumerate(test_x):\n",
        "    toks = nlp(sentence)\n",
        "    x = toks.to_array([ID])\n",
        "    x.dtype = 'long'\n",
        "    test_x[idx] = torch.tensor(x)\n",
        "\n",
        "test_x = pad_sequences(test_x, maxlen=MAX_SEQUENCE_LENGTH, dtype='long',\n",
        "                        padding='post', truncating='post')\n",
        "\n",
        "test_x = torch.tensor(test_x)\n",
        "test_y = torch.tensor(test_y)\n",
        "\n",
        "test_dataset = TensorDataset(test_x, test_y)\n",
        "test_dataloader = DataLoader(test_dataset, shuffle=True, num_workers=2)\n",
        "\n",
        "micro_f1, gold, pred = evaluate(cnn, test_dataloader, scoring='f1-micro', predict=True)\n",
        "macro_f1, gold, pred = evaluate(cnn, test_dataloader, scoring='f1-macro', predict=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoCkZRB5eTtz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f1_score(gold, pred, labels=np.unique(pred), average='macro')\n",
        "f1_score(gold, pred, average='micro')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}